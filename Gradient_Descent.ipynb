{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_Gradient_Descent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayeshsaini/Data-Lit/blob/master/Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "R1_Kjft0I8Ef",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Homework Assignment (Gradient Descent)\n",
        "Lets solve a simple python program. Copy the following into a python editor and solve the todo sections. Once you solve the given numpy arrays apply the same on a new dataset and describe why you choose the particular dataset and submit the results.\n"
      ]
    },
    {
      "metadata": {
        "id": "AV697jShI6CF",
        "colab_type": "code",
        "outputId": "03952a8c-37f5-4096-9e8d-e8a1825a4c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "# ********Python Code Starts Here*********\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  '''\n",
        "  Calculate sigmoid\n",
        "  '''\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "  '''\n",
        "  Derivative of the sigmoid function\n",
        "  '''\n",
        "  return (sigmoid(x) * (1 - sigmoid(x)))\n",
        "      \n",
        "\n",
        "learnrate = 0.5\n",
        "\n",
        "x = np.array([1, 2, 3, 4])\n",
        "y = np.array(0.5)\n",
        "\n",
        "# Initial weights\n",
        "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
        "\n",
        "### Calculate one gradient descent step for each weight\n",
        "### Note: Some steps have been consolidated, so there are\n",
        "### fewer variable names than in the above sample code\n",
        "\n",
        "\n",
        "# TODO: Calculate the node’s linear combination of inputs and weights\n",
        "h = np.dot(x, w)   # due to liner combination\n",
        "\n",
        "# TODO: Calculate output of neural network\n",
        "nn_output = sigmoid(h)\n",
        "\n",
        "# TODO: Calculate error of neural network\n",
        "error = nn_output-y\n",
        "\n",
        "# TODO: Calculate the error term\n",
        "# Remember, this requires the output gradient, which we haven’t\n",
        "# specifically added a variable for.\n",
        "      \n",
        "error_term = (sigmoid_prime(h)/len(x))*learnrate\n",
        "\n",
        "# TODO: Calculate change in weights\n",
        "del_w = w-error_term\n",
        "\n",
        "print('Neural Network output:')\n",
        "print(nn_output)\n",
        "print('Amount of Error:')\n",
        "print(error)\n",
        "print('Change in Weights:')\n",
        "print(del_w)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural Network output:\n",
            "0.6899744811276125\n",
            "Amount of Error:\n",
            "0.1899744811276125\n",
            "Change in Weights:\n",
            "[ 0.47326129 -0.52673871  0.27326129  0.07326129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1VSwoi_FWg42",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Its a bit confusing the indication of the assigment. But regarding to the second part, in my opinion, we need would select a dataset which we can classify between classes, because of the  activation function (sigmoid type) allows us to model the probability of particular class similar to in logistic regression."
      ]
    },
    {
      "metadata": {
        "id": "uUWRVhuPX57y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}